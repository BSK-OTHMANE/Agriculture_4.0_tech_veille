# ğŸŒ¾ Veille Agricole 4.0 â€“ Web Scraper & Dashboard Project

This project automates the collection and analysis of academic and alert-based content on **Agriculture 4.0**, using custom web scrapers and a fully interactive dashboard built with **Streamlit**. The data is stored in **MongoDB** and processed for credibility before visualization.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ scrape_google_alert.py                   # Fetches Google Alerts via Gmail API
â”œâ”€â”€ scrape_google_scholar.py                 # Queries Google Scholar via SerpAPI
â”œâ”€â”€ scrape_ieee.py                           # Scrapes IEEE Xplore articles
â”œâ”€â”€ scrape_springer.py                       # Scrapes SpringerLink
â”œâ”€â”€ scrape_wiley.py                          # Scrapes Wiley Online Library
â”œâ”€â”€ scrape_talkwalker/
â”‚   â”œâ”€â”€ auto_save_to_talkwalkerfolder.py     # Saves Talkwalker emails via IMAP
â”‚   â””â”€â”€ extract_informations_from_talkwalker.py  # Extracts articles from saved Talkwalker
â”œâ”€â”€ credibility_test.py                      # Filters out non-credible publications
â”œâ”€â”€ app.py                                   # Streamlit dashboard for data visualization
â”œâ”€â”€ daily_scraper_scheduler.py               # Central scheduler for daily automation
â”œâ”€â”€ requirements.txt                         # Clean dependency list
â”œâ”€â”€ .env                                     # Secret config (excluded)
â”œâ”€â”€ .gitignore                               # Git exclusion rules
â”œâ”€â”€ token.json                               # Gmail token (excluded)
â””â”€â”€ README.md                                # This file
```

---

## âš™ï¸ Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install selenium webdriver-manager pymongo schedule requests beautifulsoup4 python-dotenv streamlit
```

---

### 2. Configure Secrets

Create a `.env` file in the project root with:

```dotenv
GMAIL_TOKEN_PATH=token.json
GMAIL_CREDENTIALS_PATH=client_secret_XXXX.json
SERPAPI_KEY=your-serpapi-key
IMAP_EMAIL=youremail@example.com
IMAP_PASSWORD=your-imap-password
```

> âš ï¸ This file is ignored by Git for security.

---

## ğŸš€ Scraping & Processing

You can run scrapers individually or schedule them:

```bash
python scrape_google_alert.py
python scrape_google_scholar.py
python scrape_ieee.py
python scrape_springer.py
python scrape_wiley.py
python scrape_talkwalker/auto_save_to_talkwalkerfolder.py
python scrape_talkwalker/extract_informations_from_talkwalker.py
```

To run all daily:

```bash
python daily_scraper_scheduler.py
```

Then apply the credibility test:

```bash
python credibility_test.py
```

---

## ğŸ“Š Interactive Dashboard

The **Streamlit dashboard** is launched via:

```bash
streamlit run app.py
```

It displays:

- ğŸ“ˆ Global trends and publication growth
- ğŸŒ Geographic distribution of sources
- ğŸ“° Recent and relevant articles
- ğŸ” Top publishing domains
- â˜ï¸ Word clouds from content

### Example Screenshots

#### Vue GÃ©nÃ©rale (Overview)

![Overview](./screenshots/dashboard_01.png)

#### Contenus RÃ©cents & Pertinents

![Recent & Relevant](./screenshots/dashboard_02.png)

#### Nuage de mots & Publications par jour

![Word Cloud & Time Series](./screenshots/dashboard_03.png)

---

## ğŸ“¦ MongoDB Collections

- `google_alerts_Agriculture4.0`
- `talkwalker_alerts_Agriculture4.0`
- `ieee_agriculture_4_0_newest`
- `ieee_agriculture_4_0_relevant`
- `wiley_agriculture_4_0_newest`
- `wiley_agriculture_4_0_relevant`
- `springer_agriculture_4_0_newest`
- `springer_agriculture_4_0_relevant`
- `scholar_agriculture_4_0_newest`
- `scholar_agriculture_4_0_relevant`

---

## ğŸ§  Features

- âœ… Automated scraping with anti-bot behavior
- âœ… Secure handling of sensitive credentials
- âœ… Daily scheduling with `schedule` module
- âœ… MongoDB deduplication
- âœ… Credibility scoring & filtering
- âœ… Streamlit dashboard with dynamic filters

---

> ğŸ“ Academic project for **Veille Technologique â€“ Agriculture 4.0**  
> Created by a team of data science engineering students.
